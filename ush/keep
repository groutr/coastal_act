#!/usr/bin/env python


# standard libs
import glob, os, sys, re, time, argparse
import datetime, subprocess
from pathlib import Path

# local libs
from color import Color, Formatting, Base, ANSI_Compatible


# globals - updated during reading command line
PRJ_DIR = Path(os.path.dirname(os.path.abspath(__file__))).parent 
RUN_DIR = os.path.dirname(os.path.abspath(__file__))


def now(frmt=1):

    now = datetime.datetime.now()

    if frmt == 1:
        now = now.strftime("%Y-%m-%d %H:%M:%S")
    elif frmt == 2:
        now = now.strftime("%b. %d, %Y")
    elif frmt == 3:
        now = now.ctime()
    elif frmt == 4:
        now = now.strftime("%Y%m%d")

    return now


def exist(path):
    p = os.path.abspath(path)
    if os.path.isdir(p):
        return 1
    return 0


def found(file):

    f = os.path.abspath(file)
    if not os.path.isfile(f):
        msg = Color.F_Red + "\nFile {} not found!\n".format(file) 
        msg += Color.F_Default
        print(msg)
    return 1



# ecflow nco variables:
# **********************
# envir Set to "test" during the initial testing phase, "para" when running in parallel (on a schedule), and "prod" in production. 
# COMROOT com root directory for input/output data on current system ecFlow $COMROOT/$NET/$envir/$RUN.$PDY
# NWROOT Root directory for application, typically /nw$envir 
# GESROOT nwges root directory for input/output guess fields on current system 
# job Unique job name (unique per day and environment) 
# jobid Unique job identifier, typically $job.$$ (where $$ is an ID number) 
# jlogfile Log file for start time, end time, and error messages of all jobs  
# cyc Cycle time in GMT, formatted HH  
# DATAROOT Directory containing the working directory, often /tmpnwprd1 in production 
# HOMEmodel Application home directory, typically $NWROOT/ model.v X . Y . Z 
# SENDWEB Boolean variable used to control sending products to a web server, often ncorzdm 
# KEEPDATA Boolean variable used to specify whether or not the working directory should be deleted upon successful job completion. 

# model_ver Current version of the model; where model is the model's directory name (e.g. for $NWROOT/gfs.v12.0.0, gfs_ver=v12.0.0) 
# jjob nco variables:
# *******************
# pgmout File where stdout of binary executables may be written 
# NET Model name (first level of com directory structure) 
# RUN Name of model run (third level of com directory structure) 
# PDY Date in YYYYMMDD format
# PDYm1-7 Dates of previous seven days in YYYYMMDD format ($PDYm1 is yesterday's date, etc.)  
# PDYp1-7 Dates of next seven days in YYYYMMDD format ($PDYp1 is tomorrow's date, etc.)  
# cycle Cycle time in GMT, formatted tHHz 
# DATA Location of the job working directory, typically $DATAROOT/$jobid  
# USHmodel Location of the model's ush files, typically $HOME model /ush 
# EXECmodel Location of the model's exec files, typically $HOME model /exec 
# PARMmodel Location of the model's parm files, typically $HOME model /parm 
# FIXmodel Location of the model's fix files, typically $HOME model /fix 
# COMIN com directory for current model's input data, typically $COMROOT/$NET/$envir/$RUN.$PDY 
# COMOUT com directory for current model's output data, typically $COMROOT/$NET/$envir/$RUN.$PDY 
# COMINmodel com directory for incoming data from model model 
# COMOUTmodel com directory for outgoing data for model model 
# GESIN nwges directory for input guess fields; typically $GESROOT/$envir
# GESOUT nwges directory for output guess fields; typically $GESROOT/$envir 

# Each job in ecFlow is associated with an ecFlow script which acts like an LSF submission script, 
# 1) setting up the bsub parameters and much of 
# 2) the execution environment and 
# 3) calling the J-job to execute the job.  
# 4) All jobs must be submitted to LSF via bsub . 
# 5) It is at the ecFlow (NCO) or submission script level (development organizations) where certain 
#    environment specific variables must be set

# The purpose of the J-Job is fourfold: 
# 1) to set up location (application/data directory) variables, 
# 2) to set up temporal (date/cycle) variables, 
# 3) to initialize the data and working directories, 
# 4) and to call the ex-script. 

# The ex-script is the driver for the bulk of the application, 
# 1) including data-staging in the working directory, 
# 2) setting up any model-specific variables, 
# 3) moving data to long-term storage, 
# 4) sending products off WCOSS via DBNet and 
# 5) performing appropriate validation and error checking.  
# 6) It may call one or more utility (ush) scripts.
class NCOSystem():

    SCRIPT = 'scripts'
    SOURCE = 'sorc'
    ECF = 'ecf'
    USH = 'ush'
    EXEC = 'exec'
    PARM = 'parm'
    FIX = 'fix'
    JOBS = 'jobs'


    def __init__(self, **kwargs):

        if kwargs :
            self.run_name = "j" + kwargs['run_name']
            self.envir = kwargs['envir']
            self.COMROOT = kwargs['COMROOT']            # let's call this run_dir
            self.GESROOT = kwargs['GESROOT']            # let's make it the same level as COMROOT
            self.NWROOT = kwargs['NWROOT']              # let's call this prj_dir
            self.DATAROOT = kwargs['DATAROOT']          # let's make it /tmp<nsem> if reusable, /tmp<storm> if not
            self.NET = self.model = kwargs['model']     # storm name
            print("\nInitialized NCOSystem variables")
        else:
            print("\nSetup NCOSystem")

    
    def source_dir(self):
        return os.path.join(PRJ_DIR, self.SOURCE) 


    def script_dir(self):
        return os.path.join(PRJ_DIR, self.SCRIPT) 


    def ecf_dir(self):
        return os.path.join(PRJ_DIR, self.ECF) 


    def jjob_dir(self):
        return os.path.join(PRJ_DIR, self.JOBS) 


    def write_ecf(self, slurm):

        outfile = os.path.join(self.ecf_dir(), self.run_name + ".ecf")
        
        print("\nProcessing ecflow script %s" %outfile)

        sbatch_part = slurm.get_sbatch()
        
        lines = """\n\n%include <head.h>
%include <envir.h> 

# base name of model directory (i.e. storm)
export model={}          

# set to test during the initial testing phase, para when running 
# in parallel (on a schedule), and prod in production
export envir={}

export DEVWCOSS_USER=$(whoami)

# models input/output location
export COMROOT={}

# models restart/spinup files location
export GESROOT={}

# root directory for application, typically /nw$envir 
export NWROOT={}

# unique job name (unique per day and environment), run_name
export job={}
export jobid=$job.$$

# temp directory containing the working directory, often /tmpnwprd1 in production 
export DATAROOT={}

# log directory and files
mkdir -p $COMROOT/logs/$envir
export jlogfile=$COMROOT/logs/$envir/jlogfile.$job

# call the jjob script
$NWROOT/jobs/{}


#%manual
######################################################################
# Task script:     {}
# Last modifier:   Andre van der Westhuysen
# Organization:    NOAA/NCEP/EMC/NOS/OWP
# Date:            {}
# Purpose: To execute the job for ADC-WW3-NWM model
######################################################################
######################################################################
# Job specific troubleshooting instructions:
# see generic troubleshoot manual page
#
######################################################################
#%end""".format(self.model, self.envir, self.COMROOT, self.GESROOT,
                self.NWROOT, self.run_name, self.DATAROOT, self.run_name.upper(), 
                self.run_name, now(4))

        with open(outfile, 'w') as fptr:
            fptr.write(sbatch_part+lines)



    def write_jjob(self):
       
        outfile = os.path.join(self.jjob_dir(), self.run_name.upper())

        print("\nProcessing ecflow jjob script %s" %outfile)

  
        lines="""#!/bin/sh 
 
date
export PS4=' $SECONDS + ' 
set -x 
 
export pgm="NWPS"
export pgmout=OUTPUT.$$
 
export KEEPDATA="YES"     # TODO - to see

PDY="$(date +'%Y%m%d')"

# temp location of the job working directory, typically $DATAROOT/$jobid 
export DATA=${{DATA:-${{DATAROOT:?}}/$jobid}}
mkdir -p $DATA   
cd $DATA

####################################
# Specify NET and RUN Name and model
####################################
export NET=${{model}}
export RUN=$(echo ${{job}}|awk -F"_" '{{print $2}}')

####################################
# SENDECF  - Flag Events on ECFLOW
# SENDCOM  - Copy Files From TMPDIR to $COMOUT
# SENDDBN  - Issue DBNet Client Calls
####################################
export SENDCOM=${{SENDCOM:-YES}}
export SENDECF=${{SENDECF:-YES}}
export SENDDBN=${{SENDDBN:-NO}}

####################################
# Specify Execution Areas
####################################
export HOMEnsem=${{NWROOT}}
export FIXnsem=${{FIXnsem:-${{HOMEnsem}}/fix}}
export EXECnsem=${{EXECnsme:-${{HOMEnsem}}/exec}}
export SORCnsem=${{SORCnsem:-${{HOMEnsem}}/sorc}}
export PARMnsem=${{PARMnsem:-${{HOMEnsem}}/parm}}
export USHnsem=${{USHnsem:-${{HOMEnsem}}/ush}}
export PMnsem=${{PMnsme:-${{USHnsem}}/pm}}
export BATHYdb=${{BATHYdb:-${{FIXnsem}}/bathy_db}}
export SHAPEFILEdb=${{SHAPEFILEdb:-${{FIXnsem}}/shapefile_db}}

# Set processing DIRs here
export INPUTdir=${{INPUTdir:-${{DATA}}/input}}
export RUNdir=${{RUNdir:-${{DATA}}/run}}
export TMPdir=${{TMPdir:-${{DATA}}/tmp}}
export LOGdir=${{LOGdir:-${{DATA}}/logs}}

# Set NWPS run conditions
export DEBUGGING=${{DEBUGGING:-TRUE}}
export DEBUG_LEVEL=${{DEBUG_LEVEL:-1}}
export ISPRODUCTION=${{ISPRODUCTION:-TRUE}}
export SITETYPE=${{SITETYPE:-EMC}}

##############################################
# Define COM directory
##############################################
export COMOUT=${{COMROOT}}/${{envir}}/${{NET}}/{}.${{PDY}}
export GESOUT=${{GESROOT}}/${{envir}}/${{NET}}/{}.${{PDY}}
export GESIN=${{GESROOT}}/${{envir}}/${{NET}}/{}

# TODO - loop over dates per models
export GESINm1=${{GESROOT}}/${{envir}}/${{NET}}/{}.${{PDYm1}}
export GESINm2=${{GESROOT}}/${{envir}}/${{NET}}/{}.${{PDYm2}}

# loop over nems.configure per model, also must match run_name
export COMINatm=${{COMROOT}}/${{NET}}/${{job}}/atm
export COMINwave=${{COMROOT}}/${{NET}}/${{job}}/wave
export COMINadc=${{COMROOT}}/${{NET}}/${{job}}/adcirc
export COMINmeshdata=${{COMROOT}}/${{NET}}/${{job}}/meshdata
export COMINnwm=${{COMROOT}}/${{NET}}/${{job}}/nwm

#mkdir -p $INPUTdir $RUNdir $TMPdir $LOGdir $COMOUT $GESOUT

##############################################
# Execute the script
${{HOMEnsem}}/scripts/ex{}
##############################################

#startmsg
msg="JOB $job HAS COMPLETED NORMALLY."
postmsg $jlogfile "$msg"

if [ -e $pgmout ]; then
    cat $pgmout
fi

cd $DATAROOT

if [ "$KEEPDATA" != YES ]; then
    rm -rf $DATA
fi

date

""".format(self.run_name,self.run_name,self.run_name,self.run_name,self.run_name,self.run_name+".todo")
 
        with open(outfile, 'w') as fptr:
            fptr.write(lines)   



class NEMSConfig(NCOSystem):

    """ processes nems.configure file in PRJ_DIR 
        TODO - no need to read entire conf so far, only get 
        the model list and copy configs to run_dir 
    """

    def __init__(self, node, user_module):

        super().__init__()

        self.node = node
        self.user_module = user_module

        self.read_nems_config()
        self.read_model_config()    #TODO


    def nems_config(self):
        return os.path.join(self.source_dir(), 'nems.configure')


    def model_config(self):
        return os.path.join(self.source_dir(), 'model_configure')


    def node(self):
        return self.node

    @property
    def user_module(self):
        return self.__user_module


    @user_module.setter
    def user_module(self, module):
        self.__user_module = os.path.join(self.source_dir(),'modulefiles', self.node, module)
        return(self.__user_module)


    def earth_model_names(self):
        return self.__dict__['EARTH_component_list']


    def nems_models(self):
        return self.__dict__['NEMS_component_list']


    def read_model_config(self):
        pass


    def read_nems_config(self):

      nems_cf = self.nems_config()
      print("\nReading NEMS config file %s" %nems_cf)
         
      try:
        with open(nems_cf,'r') as fptr:
          cnt = 0; sentinel = '::'
          # saves the line # of the "::" in file nems.configure
          indx = []
          lines = fptr.readlines()
          for line in lines:
            cnt += 1
            if line.startswith(sentinel):
              indx.append(cnt)

          # cut each section of the file based on indx  
          earth_list = []; model_list = []; seq_list = []
          earth_list = lines[:indx[0]]
          model_list = lines[indx[0]:indx[len(indx)-2]]
          seq_list = lines[indx[len(indx)-2]:]

          self.process_earth_section(earth_list)
          self.process_model_section(model_list)
          self.process_runseq_section(seq_list)
          # print(self.__dict__)
      except Exception as e:
          print("\n%s" %str(e))
          sys.exit(-1)

      print("Finished reading NEMS config file nems.configure")



    def process_earth_section(self, earth_lines):

        sentinel = '::'; i=0

        # lines are not cleaned yet
        lines = [line.strip() for line in earth_lines if len(line) > 1]
        line = lines[i]

        while line:
          #print("Line {}: {}".format(i, line.strip()))

          if re.search('EARTH_component_list', line, re.IGNORECASE) :
            k,v = line.split(":")
            models = list(v.split(" "))
            self.__dict__['EARTH_component_list'] = [model.strip() for model in models if len(model) > 1]
            i += 1
            line = lines[i]

          elif re.search('EARTH_attributes', line, re.IGNORECASE) :    # do not use :: becasue it might have space between!
            self.__dict__['EARTH_attributes'] = {}
            i += 1
            line = lines[i]

            while line.find(sentinel) == -1:
              dic = self.__dict__['EARTH_attributes']
              k,v = line.split("=")
              dic[k.strip()] = v.strip()
              self.__dict__['EARTH_attributes'] = dic
              i += 1
              line = lines[i]
            break

          else:      
            i += 1
            line = lines[i]     

        print(self.__dict__['EARTH_component_list'])
        print("Pocessed EARTH_component_list")



    def process_model_section(self, model_lines):

      sentinel = '::'; cnt = 0
      lines = [line.strip() for line in model_lines if len(line) > 2] 
      line = lines[cnt]


      self.__dict__['NEMS_component_list'] = []      # introduced a new list, could be extra and overkill!!
      tmp = []

      for model in self.__dict__['EARTH_component_list']:
          while(True):
              if re.search(model+"_model", line, re.IGNORECASE) :   # do not use : becasue it might have space between!
                  k,v = line.split(":")
                  self.__dict__[model+"_model"] = v.strip()
                  tmp.append(NEMSModel(v.strip()))
                  cnt += 1
                  line = lines[cnt]

              elif re.search(model+"_petlist_bounds", line, re.IGNORECASE) :
                  k,v = line.split(":")
                  v_int = [int(i) for i in list(v.strip().split(" "))]
                  self.__dict__[model+"_petlist_bounds"] = v_int
                  cnt += 1
                  line = lines[cnt]

              elif re.search(model+"_attributes", line, re.IGNORECASE):
                  self.__dict__[model+"_attributes"] = {}
                  cnt += 1
                  line = lines[cnt]
                  while line.find(sentinel) == -1:
                      dic = self.__dict__[model+"_attributes"]
                      k,v = line.split("=")
                      dic[k.strip()] = v.strip()
                      self.__dict__[model+"_attributes"] = dic
                      cnt += 1
                      line = lines[cnt]

                  # update before breaking
                  self.__dict__['NEMS_component_list'] = tmp
                  break

              cnt += 1
              line = lines[cnt]

              #print("line %d  %s" %(cnt, line))

      print("Pocessed models_component_list")
      

    def process_runseq_section(self, runseq_lines):

      sentinel = '::'; i = 0
      lines = [line.strip() for line in runseq_lines]
      line = lines[i]

      while True :
        # for now put this as common property -TODO
        if re.search('runSeq', line, re.IGNORECASE) :
          self.__dict__["runSeq"] = []
          vals = self.__dict__['runSeq']
          i += 1
          line = lines[i]
          
          while line.find(sentinel) == -1:
            vals.append(line.strip())
            i += 1
            line = lines[i]

          self.__dict__["runSeq"] = vals
          break
        else:
          i += 1
          line = lines[i]

      print("Processed runSeq")


class NEMSBuild(NEMSConfig):

    """ This class creates a build script and copies the build file
        into PRJ_DIR, the same level as NEMS source location.
        Then runs the script to compile the system.

        It doesn't check for he correctness of the directory structure.
        It assumes all model's source are in same level of NEMS.

        # unique prj_dir sturcture is expected to be in this format per NCO standards:
        <PRJ_DIR>/     maps to NCO variable ==> ${NWROOT}
           |--- nsem??                      ==> HOMEmodel (i.e. $NWROOT/model.v X.Y.Z)  TODO - do we need this level??
           |--- sorc/                       ==> sorc
           |     |--- NEMS/   conf/   param/   modulefiles/   model_configure  nems.configure  nems_env.sh  parm  
           |     |--- NWM/  WW3/  ADCIRC/  ATMESH/
           |     |--- esmf-impi-env.sh   build.sh   nems.job
           |     |--- 
           |     |---
           |--- ecf/   jobs/   scripts/   exec/*   fix/*   parm/*   /ush*

         All stared "*" directories have defined variables in NCO in the form of "VARmodel" (i.e. USHnsem, PARMnsem).
         See files in jobs/ directory where these variables are set. 
    """

   
    def __init__(self, node, user_module):

        super().__init__(node, user_module)   

        try:
          self.write()
        except :
          return

        self.build_nems_app()


    def write(self):

        source_dir = self.source_dir()
        user_module = self.user_module

        p = os.path.join(source_dir,'build.sh')
        print("\nProcessing build script %s" %p)
 
        junk, modulefile = user_module.split(source_dir)

        lines = """#!/bin/bash\n
# Description : Script to compile NSEModel NEMS application
# Date        : {}\n
# Developer   : beheen.m.trimble@noaa.gov
# Contributors: saeed.moghimi@noaa.gov
#               andre.vanderwesthuysen@noaa.gov
#               ali.abdolali@noaa.gov
# load modules
source {}   

cd NEMS\n""".format(now(2), modulefile[1:])    # remove the '/'

        lines += '\n#clean up\n'
     
        for model in self.nems_models():
            if not found(os.path.join(source_dir, model.name)):
                return

            lines += 'make -f GNUmakefile distclean_' + \
                      model.name + ' COMPONENTS=' + \
                      '"' + model.name + '"\n'
        #except Exception as e:
        #    print("\n%s" %str(e))
        #    raise Exception
        
        lines += 'make -f GNUmakefile distclean_NEMS COMPONENTS="NEMS"\n'

        lines += '\n#make\n'
        lines += 'make -f GNUmakefile build COMPONENTS="'

        for model in self.nems_models():
            lines += model.name + ' '
        lines = lines[:-1] + '"\n'

        with open(p, 'w') as fptr:
            fptr.write(lines)

        # change mode
        subprocess.call(["chmod", "a+x", p])


        # save 
        self.build_script = p 

        print("Finished processing build script file %s" %p)



    def build_nems_app(self):

        try:
            print("\nStart compiling ............\n")

            subprocess.run(['./build.sh'], cwd=self.source_dir(), check=True)
        except subprocess.CalledProcessError as err:
            print('Error in executing build.sh: ', err)

        print("Finished compiling ............\n")



class NEMSModel:

    def __init__(self, name, **kwargs):

        self.__dict__.update({'Verbosity':'max'})  # default a must
        self.__name = name.upper()
        self.__dict__.update(**kwargs)


    @property
    def name(self):
        return self.__name


    @name.setter
    def name(self, name):
        self.__name = name


    def petlist(self,mode=1):
        """ sets and get pets lower and upper index """
        if mode == 1 :
            self.petbounds = self.__dict__.get('petlist_bounds')
        else:
            return self.__dict__.get('petlist_bounds', [-1,-1])



class NEMSInstall(NEMSConfig):
    
    # unique run_dir is constructed by this format
    """
    <RUN_DIR>/                                              ==> ${COMROOT}/
        |--- <model_name>/                                  ==> $NET
                |--- $envir/                                ==> test|para|prod 
        	        |--- <storm>/                       ==> $job
                	       |--- <input_data>
        		       |
        		       |--- <run_name>/             ==> $RUN
               			        |--- <out_data>/    ==> $COMOUT

                                                            (i.e. $COMROOT/$NET/$envir/$RUN.$PDY 

    """
    def __init__(self, region, event, run_name):

        super(NEMSInstall, self).__init__(node, user_module)   # must be same name as its parent

        self.input = event 
        self.output = (event, run_name)


    @property
    def output(self):
        return self.__output

    @output.setter
    def output(self, val):
        name = "NSEM_DATA"
        event, run_name = val
        self.__output = os.path.join(RUN_DIR, name, event, run_name)


    @property
    def input(self):
        return self.__input


    @input.setter
    def input(self, event):
        tmp = []
        for model in self.nems_models():
            name = model.name
            name += "_DATA"
            tmp.append(os.path.join(RUN_DIR, name, event))
        self.__input = tmp


    def install_dir(self):
        for p in nems_install.input:
            if not exist(p):
                print("\nCreating model input directory: {}".format(p))
                try:
                    subprocess.run(['mkdir', '-p', p ], check=True)
                except subprocess.CalledProcessError as err:
                    print('Error in creating model input directory: ', err)
     
        p = nems_install.output
        if not exist(p):
            print("\nCreating model output directory: {}\n".format(p))
            try:
                subprocess.run(['mkdir', '-p', p ], check=True)
            except subprocess.CalledProcessError as err:
                print('Error in creating model output directory: ', err)


class SlurmJob():

    # this class requires information from nems.configure,
    def __init__(self, **kwargs):

        self.__dict__ = {'account':'coastal', 'queue':'test', 'error':'nems.error', 
                         'jobname':'nems.job', 'time':420, 'output':'nems.out',
                         'mailuser':'All', 'ntasks':780, 'nodes':34,'slurm_dir':'.' 
                        }
             
        self.__dict__.update(**kwargs)

        if 'error' in kwargs:
            self.error = "j" + kwargs['error'] + ".err.log"
        if 'output' in kwargs:
            self.output = "j" + kwargs['output'] + ".out.log"
        if 'jobname' in kwargs:
            self.jobname = "j" + kwargs['jobname'] + ".job"

    def get_sbatch(self):
        return self.sbatch_part


    def print_kw(self):
        print(self.__dict__)


    def write(self):

        slurm_file = os.path.join(self.slurm_dir, self.jobname)   # TODO get the sorc from NCOSystem
        print("\nWritting slurm job file %s" %slurm_file)

        sbatch_part = """#!/bin/sh -l\n
#SBATCH -A {}
#SBATCH -q {}
#SBATCH -e {}
#SBATCH --output={}
#SBATCH --ignore-pbs
#SBATCH -J {}
#SBATCH --mail-user={}
#SBATCH --ntasks-per-node={}
#SBATCH -N {}
#SBATCH --parsable
#SBATCH -t {}""".format(self.account,self.queue,self.error,self.output,self.jobname,
                        self.mailuser, self.ntasks, self.nodes, self.time)

        # for use by other classes
        self.sbatch_part = sbatch_part

        lines = """\n\n############################### main - to run: $sbatch {} ##########################
set -x
echo $SLURM_SUBMIT_DIR		# (in Slurm, jobs start in "current dir")   
echo $SLURM_JOBID                                                     
echo $SLURM_JOB_NAME
echo $SLURM_NNODES                                             
echo $SLURM_TASKS_PER_NODE\n
echo $SLURM_NODELIST		# give you the list of assigned nodes.\n
echo 'STARTING THE JOB AT'
date\n
# change to absolute path of where you pulled the 
# NEMS and NEMS Applications. use modules.nems becasue user's
# modulefiles are copied into this with constant name.
cp -fv {}/NEMS/exe/NEMS.x NEMS.x
source {}/NEMS/src/conf/modules.nems
srun ./NEMS.x
date
""".format(self.jobname, PRJ_DIR, PRJ_DIR )

        with open(slurm_file, 'w') as f:
            f.write(sbatch_part+lines)



class NEMSRun():

    # this class requires information from NCOSystem and SlurmJob
    def __init__(self, **kwargs):

        self.__dict__.update(**kwargs)  # TODO - add check that all args are in kwargs



class BlankLinesHelpFormatter (argparse.HelpFormatter):
    # add empty line if help lines ends with \n
    # default with=54
    def _split_lines(self, text, width):

        # default _split_lines removes final \n and reduces all interior whitespace to one blank
        lines = super()._split_lines(text, width)
        if text.endswith('\n'):
            lines += ['']
        return lines



if __name__ == '__main__':

    print("\n")  # this is to print nicer
    prog = sys.argv[0]
    usage = Color.F_Blue + "%s\n"  %prog
    usage += Color.F_Red
    usage += "       %s --help | -h \n"  %prog
    usage += "       %s --region <region> --node <node> --event <event> --module <user_module> --scenario <run_name> --build <prj_dir> --install <run_dir>\n" %(prog)
    usage += "       %s -r <region> -n <node> -e <event> -u <user_module> -s <run_name> -b <prj_dir> -i <run_dir>\n" %(prog)
    usage += Color.F_Default 

    desc = '''This script builds one or more coupled models with NEMS and/or installs the system into a pre-defined location, ready to run.\n'''
    parser = argparse.ArgumentParser(prog=prog, usage=usage, add_help=True,
                                     formatter_class=BlankLinesHelpFormatter,
                                     description=desc, epilog='\n')

    parser.add_argument('-b', '--build', dest='prj_dir', type=str, default=PRJ_DIR,  
                        help='''Compiles, links, and creates libraries from the models source files using the NEMS coupler. Location of the coupled system could be provided by the user, as long as the specified location complies with the expected structure. The default build directory is {}\n'''.format(PRJ_DIR)) 
                                 
    parser.add_argument('-i', '--install', dest='run_dir', type=str, default=RUN_DIR, 
                        help='''Installs a previously NEMS built system into a NEMS compliance user defined directory. The default install directory is {}\n'''.format(RUN_DIR))

    parser.add_argument('-r', '--region', dest='region_name', type=str, default="CONUS", help='CONUS or Gulf or Atlantic - modeling region. The default region is CONUS\n')
    parser.add_argument('-n', '--node', dest='node_name', type=str, help='Name of the machine your are running this program from such as "hera". This name must be the same name as in conf/configure.nems.hera.<compiler>\n')
    parser.add_argument('-e', '--event', dest='event_name', type=str, help='A Named Storm Event Model such as "Sandy"\n')
    parser.add_argument('-u', '--module', dest='user_module', type=str, help='NEMS user module name, located in NEMS/modulefiles\n')
    parser.add_argument('-s', '--scenario', dest='run_name', type=str, help='A unique name for this model run\n')

    #parser.add_argument('-m', '--models', dest='model_list', type=str, help='Comma separated model directory names where the model source code resides at the same level of NEMS source codes. No space before and after commas. The model names are case sensative. Example: ADCIRC,WW3,NWM\n')

    args = parser.parse_args()
    print(len(sys.argv))
# ./start_workflow.py -n hera -e shinnecock -u ESMF_NUOPC -s nsem_prep -b /scratch2/COASTAL/coastal/save/NAMED_STORMS/COASTAL_ACT -i /scratch2/NCEPDEV/stmp1/Andre.VanderWesthuysen/data/com


    # local variables from comman lines
    region = node = event = user_module_name = run_name = ""

    region = args.region_name
    node = args.node_name
    event = args.event_name
    user_module_name = args.user_module
    run_name = args.run_name

    # setup nco class
    # nco = NCOSystem()                              # not needed because is superclass to NEMSConfig

    # populate NEMS model class and process nems configure files
    # needed for creation of build script
    # nems_cfg = NEMSConfig(node, user_module_name)  # not needed, superclass to NEMSBuild 

    # compile the codes and submit the job to run
    try:
      nems_build = NEMSBuild(node, user_module_name)   # TODO pickle
    except Exception:
        sys.exit(-1)

    # populate the system path
    nco_kwargs = {'envir':'test',       # para, test
                  'run_name': run_name,
                  'model'   : event,
                  'COMROOT' : RUN_DIR,                                                     # let's call this run_dir
                  'GESROOT' : '/scratch2/NCEPDEV/stmp1/Andre.VanderWesthuysen/data/nwgs',  # let's make it the same level as COMROOT
                  'DATAROOT': '/tmpnsem',                                                  # let's mak this on /tmp<storm>|/tmp<nsem>
                  'NWROOT'  : PRJ_DIR                                                      # let's call this prj_dir
                 }   
    nco = NCOSystem(**nco_kwargs)
    
    # populate nems and process nems configure files
    nems_cfg = NEMSConfig(node, user_module_name)
    sys.exit(0)
    # compiles the codes
    nems_build = NEMSBuild()   # TODO pickle

    # SlurmJob needs update by reading nems.configure, 
    slurm_args = {'account':'coastal','ntasks':'780','nnodes':'34','queue':'test',
                  'walltime':'420','jobname':run_name,'error':run_name,
                  'output':run_name,'mailuser':'??', 'slurm_dir':nco.source_dir()
                 }
    slurm = SlurmJob(**slurm_args)
    slurm.write()

    nco.write_ecf(slurm)
    nco.write_jjob()
    

    # construct model input/output paths
    # nems_install = NEMSInstall(region, event, run_name)

    # creates input/output  directory structure if they don't exist
    # nems_install.install_dir()

    # ecflow file constructions, requires infor from both
    # NCOSystem and SlurmJob
    nems_run = NEMSRun(**run_args)
    nems_run.write()

    # environment variable `$NSEMdir` that points to the location of the cloned code
    # The scratch directory with the model runs is:
    # /scratch2/NCEPDEV/stmp1/Andre.VanderWesthuysen/data
    # /scratch2/NCEPDEV/stmp1/Andre.VanderWesthuysen/data/shinnecock.atm2wav2ocn.20200528
    # Root directory for application, typically /nw$envir 
    # After the jnsem_post.ecf job is complete, the *.nc output is put in the following directory:
    # /scratch2/NCEPDEV/stmp1/Andre.VanderWesthuysen/data/com/nsem/para/shinnecock.atm2wav2ocn.20200528

    """
6) From the test run example you provided, I understood the location of each model's input files is in data/com per combination of event and run_type. Is this correct?
Not quite. The RUN_TYPE (Saeed's original "run_option") is associated with the different types of NSEM runs, and thus their *output*. So the *output* of the NSEM in COM is stored under $STORM.$RUN_TYPE. However, the inputs to NSEM are not stored like that in COM, because there is no RUN_TYPE associated with, for example, the HWRF wind fields, or the WW3 boundary conditions. So they are just stored as COM/hwrf and COM/ww3. Does that make sense?

7) How do we plan to put the model's data in its location with correct naming?
We would all need to agree that COM is the standard location for all inputs/outputs, as required by NCO. Then the HWRF runs will write their output to COM/hwrf. This in turn will become the (partial) input to NSEM. Then, when NSEM runs, its output will be written to COM/nsem/$STORM.$RUN_TYPE. From there it will go to AWS/CWWED. So if we all write to, or read from COM in our scripting, the system will work. For NSEM, the paths of these input streams are set with the env variables COMIN by storm in the jobs/ scripts.

8) Is the model compilation part of this process?
We discussed this one on our previous call. We don't strictly need to do this in our operational workflow, since we can run subsets of the NEMS app without changing the NEMS.x, as Saeed pointed out. This is also not typically done in NCO operations, since it takes a bit of time. This is of course different from our regression testing, where the code needs to be compiled each time we go through the CI workflow. This is part of the compsets we have in our old VLab version of our app, and it still needs to be brought over to our Git version.

9) How did you produce the jjob file? It seems that there are many variables that need to be updated per unique run?
The jjob file is a standard template constructed manually. The runtime variables that need to be updated come from the env variables such as $STORM and $RUN_TYPE. Are there others you can think of that we need to incorporate? """

